model_list:
  # Route the specific ID requested in the logs
  - model_name: "glm-4.7"
    litellm_params:
      model: "zai/{{ llm_model }}"
      api_base: "{{ llm_url }}"
      api_key: "os.environ/TARGET_LLM_KEY"

  - model_name: "glm-4.5v"
    litellm_params:
      model: "zai/{{ llm_model }}"
      api_base: "{{ llm_url }}"
      api_key: "os.environ/TARGET_LLM_KEY"

  - model_name: "glm-4.5-flash"
    litellm_params:
      model: "zai/{{ llm_model }}"
      api_base: "{{ llm_url }}"
      api_key: "os.environ/TARGET_LLM_KEY"

  # Catch-all
  - model_name: "*"
    litellm_params:
      model: "zai/{{ llm_model }}"
      api_base: "{{ llm_url }}"
      api_key: "os.environ/TARGET_LLM_KEY"

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  max_parallel_requests: 10
  # Allow all routes to ensure no 404s on Anthropic-specific endpoints
  allowed_routes:
    - /v1/messages
    - /v1/complete
    - /v1/chat/completions

litellm_settings:
  request_timeout: 600
  drop_params: true
  num_retries: 2