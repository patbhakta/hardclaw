{% if llm_provider == 'ollama' %}
  {% set lite_provider = 'ollama' %}
{% elif llm_provider == 'anthropic' %}
  {% set lite_provider = 'anthropic' %}
{% elif llm_provider == 'openai' %}
  {% set lite_provider = 'openai' %}
{% elif llm_provider == 'openrouter' %}
  {% set lite_provider = 'openrouter' %}
{% elif llm_provider == 'z.ai' %}
  {% set lite_provider = 'openai' %}
{% elif llm_provider == 'gemini' %}
  {% set lite_provider = 'gemini' %}
{% elif llm_provider == 'openai_compatible' %}
  {% set lite_provider = 'openai' %}
{% else %}
  {% set lite_provider = 'ollama' %} 
{% endif %}

  # Gemini Routes
  - model_name: "gemini/*"
    litellm_params:
      model: "gemini/$model"
      api_key: "os.environ/GEMINI_API_KEY"
      api_base: "os.environ/GEMINI_API_BASE"
    model_info:
      base_model: "gemini/*"

  # OpenRouter Routes
  - model_name: "openrouter/*"
    litellm_params:
      model: "openrouter/$model"
      api_key: "os.environ/OPENROUTER_API_KEY"
    model_info:
      base_model: "openrouter/*"

  # Anthropic Routes
  - model_name: "claude-*"
    litellm_params:
      model: "anthropic/$model"
      api_key: "os.environ/ANTHROPIC_API_KEY"
    model_info:
      base_model: "anthropic/*"

  # OpenAI Routes
  - model_name: "gpt-*"
    litellm_params:
      model: "$model"
      api_key: "os.environ/OPENAI_API_KEY"

  # Primary Deployment Fallback (Legacy names support)
  - model_name: "primary-fallback"
    litellm_params:
      model: "{{ lite_provider }}/{{ llm_model }}"
      {% if llm_url | length > 0 %}
      api_base: "{{ llm_url }}"
      {% endif %}
      api_key: "os.environ/TARGET_LLM_KEY"

  # Catch-all using primary deployment settings
  - model_name: "*"
    litellm_params:
      model: "{{ lite_provider }}/{{ llm_model }}"
      {% if llm_url | length > 0 %}
      api_base: "{{ llm_url }}"
      {% endif %}
      api_key: "os.environ/TARGET_LLM_KEY"

general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  max_parallel_requests: 10
  # Allow all routes to ensure no 404s on Anthropic-specific endpoints
  allowed_routes:
    - /v1/messages
    - /v1/complete
    - /v1/chat/completions

litellm_settings:
  request_timeout: 600
  drop_params: true
  num_retries: 2